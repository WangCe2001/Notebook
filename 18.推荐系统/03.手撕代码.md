# 一.Transformer

## 1.多头注意力机制

```python
import torch
from torch import nn

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # 每个头的维度，二者必须整除
        
        # 初始化 Q、K、V 的投影矩阵，将输入词向量线性变换为 Q、K、V，维度保持一致
        self.q_linear = nn.Linear(hidden_size, hidden_size) 
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        
        # 输出线性层，将拼接后的多头注意力输出变换为所需的输出维度，这里维度保持一致
        self.o_linear = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, hidden_state, causal_mask=None, padding_mask=None):
        # hidden_state 形状: (batch_size, seq_len, hidden_size)
        batch_size = hidden_state.size(0)  # 获取批量大小

        # 计算 Q、K、V，线性变换，得到形状：(batch_size, seq_len, hidden_size)
        query = self.q_linear(hidden_state)
        key = self.k_linear(hidden_state)
        value = self.v_linear(hidden_state)
        
        # 将每个头的维度拆分出来，得到形状：(batch_size, num_heads, seq_len, head_dim)
        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        # 计算注意力分数，attention_scores 形状: (batch_size, num_heads, seq_len, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) \
        / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        # 添加因果注意力掩码（seq_len, seq_len），掩码位置（1）的值为负无穷，自动广播
        if causal_mask is not None:
            attention_scores += causal_mask * -1e9
        
        # 添加填充位置的掩码，每个句子不一样（batch_size, seq_len)
        if padding_mask is not None:
            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, seq_len)
            attention_scores += padding_mask * -1e9
            
        # 对注意力分数进行归一化，得到注意力概率
        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)
        # 如果有 dropout 操作就加在这，self.dropout(attention_probs)，也可以在函数外面加

        # 计算注意力输出，通过注意力概率加权值
        output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, head_dim)
        
        # 对多头注意力输出进行拼接，将形状调整为 (batch_size, seq_len, hidden_size)
        # 先 output.transpose(1, 2) 将 num_heads 和 seq_len 维度转置
        output = output.transpose(1, 2).view(batch_size, -1, self.head_dim * self.num_heads)
        
        # 通过线性层将拼接后的输出变换为所需的输出维度
        output = self.o_linear(output)  # (batch_size, seq_len, hidden_size)
        return output

def test_MHA():
    batch_size = 128
    seq_len = 512
    hidden_size = 1024
    num_heads = 8
    
    # 随机生成输入数据
    hidden_state = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)
    
    # 生成因果掩码（下三角矩阵），这里就不刻意生成 padding_mask
    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    
    # 创建多头注意力模块
    mha = MultiHeadAttention(hidden_size, num_heads)
    
    # 计算多头注意力输出
    output = mha(hidden_state, causal_mask=causal_mask)
    
    print("Input shape:", hidden_state.shape)
    print("Output shape:", output.shape)
    
if __name__ == "__main__":
	test_MHA()
```

# 二.机器学习

## 1.损失函数

### 1.1 均方误差（Mean Squared Error,MSE）

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

```python
import numpy as np
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
```

### 1.2 二元交叉熵损失函数（Binary Cross Entropy,BCE）

$$
L = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$

```python
import numpy as np
def binary_cross_entropy(y_true, y_pred):
    """
    计算二元交叉熵损失（需配合 Sigmoid 使用）
    :param y_true: 二分类的真实标签（0 或 1），形状 (n_samples, n_classes)
    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)
    :return: 标量损失值
    """
    # 应用 Sigmoid 将 logits 转换为概率
    sigmoid_output = 1 / (1 + np.exp(-y_pred))
    # 避免 log(0) 导致的数值问题
    epsilon = 1e-7
    clipped = np.clip(sigmoid_output, epsilon, 1 - epsilon)
    # 计算每个样本每个类别的损失
    loss_per_element = - (y_true * np.log(clipped) + (1 - y_true) * np.log(1 - clipped))
    # 对所有元素取平均损失
    return np.mean(loss_per_element)
```



### 1.3 交叉熵损失函数(Cross Entropy,CE)

$$
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
$$

```python
import numpy as np
def cross_entropy(y_true, y_pred):
    """
    计算交叉熵损失（需配合 Softmax 使用），带数值稳定处理
    :param y_true: one-hot 编码的真实标签，形状 (n_samples, n_classes)
    :param y_pred: 模型输出的 logits，形状 (n_samples, n_classes)
    :return: 标量损失值
    """
    # 数值稳定处理：减去最大值防止指数爆炸
    exps = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))
    softmax_output = exps / np.sum(exps, axis=1, keepdims=True)
    # 避免 log(0) 导致数值问题
    epsilon = 1e-7
    clipped = np.clip(softmax_output, epsilon, 1 - epsilon)    
    # 只计算真实类别对应的损失
    n_samples = y_true.shape[0]
    log_likelihood = -np.log(clipped[range(n_samples), y_true.argmax(axis=1)])
    return np.mean(log_likelihood)
```

## 2.激活函数

### 2.1 Sigmoid

$$
σ(x)= \frac{1}{1+e^{-x}}
$$

```python
import numpy as np
def sigmoid(x):
    x = np.clip(x, -50, 50)
    return 1 / (1 + np.exp(-x))
```

### 2.2 Tanh（双曲正切）

$$
tank(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

```python
import numpy as np
def tanh(x):
    return np.tanh(x)
```

### 2.3 ReLU

$$
ReLU(x)=max(0,x)
$$

```python
import numpy as np
def relu(x):
    return np.maximum(0, x)
```

### 2.4 Leaky ReLU

$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases}
$$

```python
import numpy as np
def Leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x , alpha * x)
```

### 2.5 Softmax

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

```python
import numpy as np
def softmax(x):
    exps = np.exp(x - np.max(x, axis=1, keepdims = True))
    return exps / np.sum(exps, axis = -1, keepdims = True)
```

