# 录音面经

#### **问题 1：协同过滤（ItemCF / UserCF）的实现细节**



面试官通过提问权重设计、热门物品降权等方式，考察您对协同过滤算法细节的掌握。

- **您回答的核心点**：

  - 

    **ItemCF**: 提到了基于时间间隔、物品相似度等加权，并对热门物品通过 `log` 函数进行降权 1。

    

    

  - 

    **UserCF**: 提到了计算用户间的相似度，并同样对热门物品降权 2。

    

    

- **回答评价**：您的回答基本正确，覆盖了关键的降权思想。但面试官进一步追问了 UserCF 中热门物品问题更严重的原因，这部分可以回答得更深入。

- 参考答案：

  协同过滤的核心是“人以群分”和“物以类聚”。

  - **ItemCF（物品协同过滤）**：
    - **核心思想**：它认为用户喜欢的物品是相似的。实现上分为两步：
      1. **离线计算物品相似度**：遍历所有用户的行为序列，构建物品与物品的“共现矩阵”。例如，如果物品 A 和 B 同时被很多用户喜欢，它们的相似度就高。
      2. **在线推荐**：获取用户最近喜欢的物品列表，然后找到与这些物品最相似的 Top-K 个物品，推荐给用户。
    - **热门物品降权（Why & How）**：像《新华字典》这样的物品，几乎和任何书都一起出现过，但它对判断“相似性”几乎没有帮助。如果不降权，任何物品都可能和它有很高的相似度，导致推荐列表被热门但无意义的物品污染。您的回答 `log(N(i)+1)` 是一个非常标准的降权公式（其中 N(i) 是喜欢物品 i 的用户数），这很好。
  - **UserCF（用户协同过滤）**：
    - **核心思想**：它认为“相似”的用户会有相似的兴趣。
    - **热门物品问题为何更严重**：在 UserCF 中，两个用户可能仅仅因为都买过《新华字典》、iPhone 等国民级产品，就被算法判定为“相似用户”，但他们的真实兴趣可能天差地别。**热门物品会成为计算用户相似度的“噪声”**，极大地降低了找“真实近邻”的准确性，因此在 UserCF 中对热门物品降权更加重要。

------



#### **问题 2：双塔模型中的负采样（Negative Sampling）策略**



这是面试中的一个**核心考察点**，也是一个非常经典的“陷阱题”。

- 

  **您回答的核心点**：承认自己采用了**全局随机负采样** 3。

  

  

- **回答评价**：这是一个非常诚实的回答，但也是双塔模型实践中最基础、效果也最差的一种方法。面试官问这个问题，就是想考察您是否了解随机负采样的弊端以及业界更优的做法。

- 参考答案：

  双塔模型训练的关键在于如何构建有效的正负样本对。

  - 

    **正样本**：通常是用户真实发生过行为（如点击、观看、购买）的物品 4。

    

    

  - **负样本**：

    - **全局随机负采样的弊端**：您提到的随机采样，会导致一个严重的问题——**“Easy Negative”过多**。因为大量的物品都是用户永远不会感兴趣的冷门或无关物品，模型可以轻易地将它们与正样本区分开，导致模型学不到什么有用的信息，无法对那些“有点像但用户没点”的“Hard Negative”进行有效区分。
    - **更优的负采样策略**：
      1. **曝光未点击样本**：这是最理想的负样本，它们是用户看到了但明确选择不点击的，属于高质量的“Hard Negative”。在您的项目中，虽然数据里没有直接提供，但在工业界这是负样本的核心来源。
      2. **Batch内负采样 (In-Batch Negatives)**：在训练的一个 mini-batch 中，对于一个用户的正样本，将该 batch 内其他用户的正样本作为它的负样本。这种方法简单高效，且因为这些物品都是其他用户喜欢的，它们通常比随机采样的物品更“像样”，难度更高。
      3. **基于流行度的负采样 (Popularity-based Sampling)**：按照物品的流行度进行采样，越热门的物品越有可能被选为负样本。这模拟了真实世界中用户总是在热门物品中做选择的场景，也是一种增加负样本难度的方法。

------



#### **问题 3：样本选择偏差（Sample Selection Bias）问题**



面试官提到“用户能看到的都是你推荐的”，这是在考察您是否意识到推荐系统固有的“幸存者偏差”问题。

- **您回答的核心点**：您意识到了这个问题，但没有给出具体的解决方案。

- **回答评价**：能意识到问题本身已经不错了，但如果能讲出一些业界的解决方案，会是巨大的加分项。

- 参考答案：

  您说的非常对，模型只能学习到那些被“曝光”给用户的数据，而那些从未被推荐过的物品，模型永远不知道用户是否会喜欢它们。这就是典型的样本选择偏差。解决这个问题是提升模型泛化能力的关键，业界有一些常用方法：

  1. **倾向性得分（Propensity Score）**：这是一种“无偏”的评估思想。对于一个曝光并被点击的样本，我们在计算损失时给它一个权重，这个权重是它当初被曝光概率的倒数（即 `1/p`）。直观理解就是，一个本身很难被推荐（曝光概率 p 很小）但却被用户点击了的物品，说明用户是“真爱”，我们应该给它更高的权重让模型好好学习。
  2. **数据增强与探索**：在线上流量中切出一小部分（比如5%）用于“探索”，随机推荐一些物品，收集用户对这些“新”物品的反馈。这样可以获取到更全面的用户偏好数据，用来校正模型的偏差。
  3. **使用能够减轻偏差的模型**：例如，一些因果推断模型会试图去学习物品本身属性和用户反馈之间的“因果关系”，而不是被曝光行为所干扰的“相关性”。

------



#### **问题 4：为什么需要“召回 -> 排序”的多阶段架构？**



- 

  **您回答的核心点**：您准确地指出了这是**效率与效果的权衡（Trade-off）**。在海量物品库中，无法对所有物品进行复杂模型打分，因此需要召回层快速筛选 5。

  

  

- **回答评价**：回答得非常完美，直击要害，体现了您对工业级推荐系统架构的深刻理解。

------

<h4>**问题 5（逻辑/算法题）：手动计算 AUC**</h4>

面试官给了一组模型预测分 (score) 和真实标签 (label)，要求计算 AUC。

- **您回答的核心点**：您在录音中开始排序，并尝试回忆公式，但最后没有完成计算。

- **回答评价**：这是考察对 AUC 物理意义理解的经典题目。知道要排序是第一步，关键是后续的计算方法。

- 参考答案：

  AUC 的物理意义是：随机抽取一个正样本和一个负样本，正样本的预测分数大于负样本的预测分数的概率。 手动计算最简单的方法是：

  1. **数据准备**：将 `score` 和 `label` 组合并按 `score` 从高到低排序。
     - 给定的数据：`scores = [0.1, 0.4, 0.35, 0.8]`, `labels = [0, 0, 1, 1]`
     - 排序后：
       - (0.8, 1)  <- 正样本
       - (0.4, 0)  <- 负样本
       - (0.35, 1) <- 正样本
       - (0.1, 0)  <- 负样本
  2. **计算方法**：遍历所有“正负样本对”，看有多少对满足“正样本分数 > 负样本分数”。
     - 我们有两个正样本 `P1(0.8), P2(0.35)` 和两个负样本 `N1(0.4), N2(0.1)`。
     - 总共有 `2 * 2 = 4` 个正负样本对。
     - **P1 vs N1**: `0.8 > 0.4` (满足)
     - **P1 vs N2**: `0.8 > 0.1` (满足)
     - **P2 vs N1**: `0.35 < 0.4` (不满足)
     - **P2 vs N2**: `0.35 > 0.1` (满足)
     - 在 4 个样本对中，有 3 对满足条件。
  3. **得出结果**：所以 AUC = `3 / 4 = 0.75`。

  还有一个更快的公式化方法：对排序后的列表，计算所有正样本的“排名”（rank，假设排名从1开始），然后使用公式：

  

  $$AUC = \frac{\sum_{i \in \text{正样本}} \text{rank}_i - \frac{M(M+1)}{2}}{M \times N}$$

  - M 是正样本数量 (2)，N 是负样本数量 (2)。
  - 正样本排名为 1 和 3。
  - $AUC = \frac{(1+3) - \frac{2(2+1)}{2}}{2 \times 2} = \frac{4 - 3}{4} = 0.25$
  - *（这里发现公式记错了，或者排名方式不对，手动检查）*
  - **纠正方法**：正确的公式是基于负样本排名的。对每个正样本，计算它排在多少个负样本前面。
    - `P1(0.8)` 排在 `N1` 和 `N2` 两个负样本前面。
    - `P2(0.35)` 排在 `N2` 一个负样本前面。
    - 总共是 `2 + 1 = 3`。
    - 总的正负样本对是 `M * N = 2 * 2 = 4`。
    - AUC = `3 / 4 = 0.75`。 **这个方法更直观，不易出错。**



### **给您的总体建议**



1. **深入思考“Why”**：您的项目经历很扎实，但面试官更关心您做出技术决策背后的原因。在准备项目时，不仅要清楚“做了什么”，更要反复思考“**为什么这么做**”、“**有没有更好的方法**”、“**我用的方法有什么缺点**”。负采样和样本偏差就是典型的例子。
2. **夯实算法细节**：对于 AUC、协同过滤降权等，要理解其背后的数学/物理意义。这能帮助您在遇到变种问题（如手动计算 AUC）时，从第一性原理出发解决问题，而不是仅仅依赖背诵。
3. **关注业界前沿**：面试官提到“业界现在怎么做”，说明他们期待候选人有更广阔的视野。建议多阅读一些技术博客、顶会论文摘要（如 RecSys、KDD），了解当前推荐系统领域流行的方法和正在解决的问题。

您的基础很好，这次面试暴露出的问题都非常具体，是可以通过针对性学习快速弥补的。祝您在接下来的求职中取得好成绩！

# 1.双塔模型

面试官问及这个问题时，你可以先给出一个**提纲挈领的核心论点**，然后再分点详细阐述。

“这是一个非常好的问题。双塔模型和DIN模型之所以采用不同的损失函数（`SampledSoftmaxLoss` vs. `Sigmoid`+`BinaryCrossEntropy`），其根本原因在于它们所处的**推荐阶段**和要解决的**核心任务**完全不同：

- **双塔模型（召回阶段）**：它的任务是**“大海捞针”**。它需要从**百万级**的全体物品库中，快速找出数百个可能相关的物品。
- **DIN模型（排序阶段）**：它的任务是**“精挑细选”**。它只需要在召回送来的**几百个**候选物品中，进行精准的排序。

**正是这个‘候选集规模’的巨大差异，决定了它们必须采用不同的训练策略。**”

------



### **1. 为什么双塔模型使用的是`SampledSoftmaxLoss`？**



“双塔模型作为**召回模型**，它的训练目标是学习出在**全局范围**内都有区分度的Embedding。

- **理想情况**: 我们希望模型在训练时，对于一个正样本`(用户U, 物品A)`，能够将`物品A`与**所有其他999,999个物品**区分开。这本质上是一个有**一百万个类别**的**多分类问题**。
- **现实困境**: 直接在这个百万级的类别上计算标准的Softmax损失，其分母需要对所有一百万个物品都计算一遍得分，这在计算上是**完全不可行**的。
- **`SampledSoftmaxLoss`的作用**:
  - 它正是为了解决这个‘不可能完成的任务’而被设计的。它将一个**‘百万选一’**的难题，巧妙地转化为了一个**‘几十选一’**的简单选择题。
  - 在每个训练步，它为正样本`A`，动态地从全局物品库中**随机采样**几十个负样本（比如`[X, Y, Z,...]`）。然后，它只在这个**极小的集合**上计算Softmax和交叉熵损失。
  - **因此，`SampledSoftmaxLoss`是一种高效的近似算法，它使得在海量候选池上训练一个‘多分类’的召回模型成为可能。**”



#### **为什么这里不能使用Sigmoid？**



“理论上也可以用Sigmoid，但效果和效率会差很多。如果用Sigmoid，我们就把问题看作了多个独立的二分类问题。对于一个正样本`(U, A)`，我们需要为它搭配一个负样本`(U, B)`。

- **问题在于**: 负样本B从哪里来？如果只随机采一个B，那模型只学会了‘A比B好’，但它不知道A是不是也比C, D, E...好。为了达到好的效果，我们需要为每个正样本搭配**海量的负样本对**，这在数据构造和训练效率上都非常低下。
- 而`SampledSoftmaxLoss`通过一次计算，就让正样本**同时与几十个负样本进行了‘对比’**，学习信号更强，效率也更高。”

------



### **2. 为什么DIN模型使用的是Sigmoid (配合`binary_crossentropy`)？**



“DIN模型作为**排序模型**，它面临的任务环境与双塔完全不同。”

- **任务已简化**: 召回阶段已经完成了‘大海捞针’，此时DIN的输入，只是一个**几百个物品**的候选列表。它**不再需要**从全局百万物品中去区分谁好谁坏。
- **任务变为Pointwise**: DIN的任务，是**逐一地(Pointwise)\**评估这个列表里的每一个物品，并为每个物品都给出一个\**精准的点击率预估(pCTR)**。
- **`Sigmoid`的完美匹配**: 这个任务本质上就是一个标准的**二元分类问题**：‘对于这个`{用户, 物品}`对，它会被点击(1)还是不会被点击(0)？’
  - **Sigmoid函数**的作用，就是将模型内部计算出的任意分数（logit），完美地映射到一个`[0, 1]`区间的**概率值**上。
  - 而**`binary_crossentropy`**损失函数，则是衡量这个预测概率与真实标签(0或1)之间差距的**标准工具**。



#### **为什么DIN不使用`SampledSoftmaxLoss`？**



“因为**没必要**。`SampledSoftmaxLoss`是为了解决‘类别数量过大’的问题。而在排序阶段，候选物品只有几百个，类别数量很小。我们完全可以：

1. **直接计算全量Softmax (Listwise)**: 像`LGBMRanker`那样，在几百个物品的列表上直接优化排序。
2. **或者，采用更简单的Pointwise方法**: 逐一判断每个物品的点击概率，然后排序。

在我的项目中，DIN采用的就是第二种**Pointwise**方法，它更简单、更直接，并且已经被证明在CTR预估任务中非常有效。”

------



### **总结：两个模型在训练上的根本不同**



| **对比维度**   | **双塔模型 (召回)**                         | **DIN模型 (排序)**                    |
| -------------- | ------------------------------------------- | ------------------------------------- |
| **训练目标**   | 学习**全局有区分度**的Embedding             | 学习对**少量候选者**的**精准打分**    |
| **问题建模**   | **超大规模多分类** (N选1, N=百万级)         | **二元分类** (是/否)                  |
| **负样本空间** | **全局物品库**                              | **召回列表内的其他物品**              |
| **损失函数**   | **`SampledSoftmaxLoss`** (近似多分类交叉熵) | **`BinaryCrossEntropy`** (二元交叉熵) |
| **输出激活**   | **无** (原始分数直接给损失函数)             | **Sigmoid** (将分数转换为概率)        |

“所以，总结来说，这两个模型在训练上的根本不同，源于它们**所处阶段和任务的不同**。

- **双塔模型**为了从‘**汪洋大海**’中高效地学习，必须采用`SampledSoftmaxLoss`这种近似方法。
- 而**DIN模型**则是在一个‘**小池塘**’里进行精细操作，因此可以使用`Sigmoid`+`BinaryCrossEntropy`这种标准的、为精准预测而生的Pointwise方法。”



# 2.召回和排序的正负样本是什么

当面试官问：“你项目中召回和排序阶段的正负样本，分别是什么？”

你可以这样回答：

“这是一个很好的问题。在我的项目中，召回和排序两个阶段，虽然都依赖于正负样本进行监督学习，但它们的**样本定义、来源和核心目的**是完全不同的，这恰恰体现了两个阶段的不同职责。”

- **召回阶段**的目标是**“大海捞针”**，它的任务是从**海量的、宽泛的**负样本中，学习用户兴趣的大致方向。
- **排序阶段**的目标是**“精挑细选”**，它的任务是从**少量、高度相关的**负样本中，学习用户兴趣的细微差别。

下面我来分别详细解释：

------



### **1. 召回阶段 (Recall Stage)**



“在我的项目中，召回阶段的主力是**双塔模型**，它的正负样本定义如下：”

- 

  #### **正样本 (Positive Sample)**

  

  - **定义**: **用户在历史行为序列中，真实点击的下一个物品**。
  - **实现方式**: 正如我代码中的`gen_data_set`函数所示，我们通过**滑动窗口**的方式来生成。例如，如果一个用户的历史是`[A, B, C, D]`，我们就会生成：
    - `(历史=[A, B], 正样本=C)`
    - `(历史=[A, B, C], 正样本=D)`
    - ...等等。

- 

  #### **负样本 (Negative Sample)**

  

  - **定义**: **从整个物品库中，随机抽取的、用户未交互过的物品**。
  - **实现方式**: 在我的双塔模型训练中，负样本并**不是**在数据预处理阶段生成的。而是由损失函数 **`SampledSoftmaxLoss`**，在**模型训练的每一刻，实时、动态地**从全局物品库中随机抽取的。
  - **核心任务**: 召回模型的训练任务可以理解为：“**在1个正样本和几百个（甚至更多）随机负样本中，挑出那个正样本**”。它学习的是一种**“粗粒度”**的区分能力，目标是让用户向量与正样本向量的距离，远小于与那些“八竿子打不着”的随机物品的距离。

**类比**: 召回阶段的学习，就像是让模型学会区分**“苹果”**和**“汽车、房子、铅笔”**。

------



### **2. 排序阶段 (Ranking Stage)**



“当召回阶段为每个用户筛选出几百个候选物品后，排序阶段就要开始进行‘精加工’。它的正负样本定义如下：”

- 

  #### **正样本 (Positive Sample)**

  

  - **定义**: 在**召回产生的候选列表**中，那个**恰好是**用户未来真实点击的物品（即我们通过‘留一法’留出的那个‘标准答案’）。
  - **实现方式**: 在特征工程阶段，我们将召回列表与用户的“最后一次点击”记录进行匹配。匹配上的那一个`{用户, 物品}`对，就被标记为`label=1`。通常，对于每个用户，在他的候选列表中，**最多只有一个正样本**。

- 

  #### **负样本 (Negative Sample)**

  

  - **定义**: 在**召回产生的候选列表**中，**除了那个唯一的正样本之外的所有其他物品**。
  - **实现方式**: 候选列表中，所有未被匹配上“标准答案”的`{用户, 物品}`对，都被标记为`label=0`。
  - **核心任务**: 排序模型的训练任务可以理解为：“**在这一小撮（几百个）都挺相关的候选物品中，精准地识别出用户最最喜欢的那一个**”。它学习的是一种**“细粒度”**的排序能力。

**类比**: 排序阶段的学习，就像是让模型学会区分**“红富士苹果”**和**“花牛苹果、青苹果、国光苹果”**。这里的负样本，不再是“汽车、房子”，而是与正样本非常相似、极具迷惑性的“强负样本”(Hard Negatives)。

------



### **总结与对比**



为了让您更清晰地理解，这是一个总结性的对比表格：

| **特性**       | **召回阶段 (Recall Stage)**                | **排序阶段 (Ranking Stage)**                 |
| -------------- | ------------------------------------------ | -------------------------------------------- |
| **正样本来源** | 用户行为**序列中的下一个**真实点击         | **召回列表**中与**最终答案**匹配的那一个     |
| **负样本来源** | **整个物品库**中的随机物品                 | **召回列表**中的其他物品                     |
| **负样本特点** | **宽泛、随机、“简单”**                     | **相关、高质量、“困难”** (Hard Negatives)    |
| **学习目标**   | **“大海捞针”**：从海量无关物品中找到相关的 | **“精挑细选”**：从少量相关物品中找到最好的   |
| **模型职责**   | 学习**泛化**的兴趣，保证**召回率**和覆盖度 | 学习**细微**的偏好，保证**精准率**和排序质量 |

“因此，通过这两套截然不同的正负样本定义，召回和排序两个阶段得以各司其职、有效协作。召回模型负责‘广’，保证不漏掉任何可能的机会；排序模型负责‘精’，保证呈现给用户的都是优中选优的结果，最终构成了我们项目完整而高效的推荐流程。”



您的直觉是完全正确的：**如果不考虑任何限制，理论上最完美的推荐系统，就是用最强大的排序模型，去给整个物品库里的每一件物品都打一遍分，然后排序。**

但现实是，我们**永远无法忽略**计算性能和速度的限制。下面我将为您详细解释为什么“召回”阶段是不可或缺的，以及如何回答这个问题。

------



### **面试回答策略**

# 3.为什么还需要召回阶段？

当面试官问：“既然排序模型那么强大，为什么还需要召回阶段？”

你可以这样回答：

“您提的这个问题非常深刻，它触及了现代大规模推荐系统架构设计的核心基石。理论上，如果算力无限，直接用最复杂的排序模型处理所有物品确实是最佳方案。但在现实世界中，我们必须面对一个**‘不可能三角’**：**海量的物品库、复杂的排序模型、以及毫秒级的响应时间**，这三者无法同时满足。

因此，‘**召回-排序’二级架构**，正是为了解决这个核心矛盾而诞生的、业界公认的最优工程实践。**召回和排序，它们不是‘谁更好’的竞争关系，而是‘分工协作、缺一不可’的合作关系。**”

------



### **1. 核心矛盾：“不可能三角”**



- **海量物品库 (Massive Corpus)**: 现代推荐系统动辄需要从**百万、千万甚至上亿**的物品中进行选择。
- **复杂排序模型 (Complex Models)**: 我们的排序模型，如**DIN**或**LGBMRanker**，使用了极其丰富的特征，模型本身也非常复杂，单次预测就需要相当的计算量。
- **实时响应 (Real-time Latency)**: 用户几乎没有耐心等待。一次推荐请求，从发起到返回结果，必须在**100-200毫秒**内完成。

让我们做一个简单的计算：假设为1个物品打分需要0.1毫秒，那么为100万个物品打分就需要 0.1ms * 1,000,000 = 100,000ms = 100秒。

—— 这是用户绝对无法接受的。

**结论**: **直接用复杂模型对全量物品进行排序，在工程上是完全不可行的。**

------



### **2. 解决方案：“召回-排序”二级漏斗架构**



为了解决这个矛盾，我们设计了“召回-排序”这样一个“**层层筛选的漏斗模型**”。

**我喜欢用一个“大学招生”的类比来解释它：**



#### **第一阶段：召回 (Recall) - “海选/高考”**



- **核心目标**: **快！广！** —— **快速地**从**海量**的申请人（全量物品）中，筛选出一批**可能合格**的候选人。宁可“错杀”一千，不可“放过”一个。
- **所用模型**: 必须是**简单、高速**的算法。比如我项目中的：
  - **ItemCF/UserCF**: 基于预计算好的相似度矩阵，进行极快的查表。
  - **双塔模型**: 将复杂的匹配过程，拆解为一次实时的用户向量计算和一次高效的Faiss向量检索。
- **特征使用**: 只使用少量、关键的特征（如ID、类别）。
- **类比**: 就像大学招生中的“高考”或“机考筛选”。我们用一套标准化的、能快速评分的试卷，从几百万考生中，快速筛选出成绩过线的5000人。这个阶段，我们不关心每个人的个性、特长，只关心“及格线”。
- **产出**: 一个**几百到几千**的、相对粗糙但覆盖全面的**候选集**。



#### **第二阶段：排序 (Ranking) - “精排/面试”**



- **核心目标**: **准！** —— 对已经大大缩减的候选集，进行**精准的、个性化的**打分和排序。
- **所用模型**: 可以“奢侈地”使用**复杂、强大但稍慢**的模型。比如我项目中的：
  - **LGBMRanker**: 使用几百个精细的交叉特征。
  - **DIN**: 使用能捕捉用户动态兴趣的注意力网络。
- **特征使用**: 使用所有能用上的丰富特征（用户画像、物品画像、上下文、交叉特征等）。
- **类比**: 就像招生中的“自主招生面试”。招生官（排序模型）会对通过海选的5000名学生，进行**一对一的、深入的面试**。他们会仔细审阅每个学生的全部材料（丰富的特征），考察他们的方方面面，最终给出一个最精准的“录取顺位”。
- **产出**: 一个**几十个**物品的、**排序精准**的、可以直接呈现给用户的**最终推荐列表**。

------



### **3. 总结与对比**



| **对比维度** | **召回阶段 (Recall Stage)**              | **排序阶段 (Ranking Stage)**               |
| ------------ | ---------------------------------------- | ------------------------------------------ |
| **核心目标** | **效率优先**，保证**召回率**和**多样性** | **效果优先**，追求**精准率**和**排序质量** |
| **处理范围** | **全量**物品库（百万级 → 数百级）        | **小候选集**（数百级 → 数十级）            |
| **模型特点** | **简单、高速** (ItemCF, 双塔模型等)      | **复杂、精准** (LGBMRanker, DIN等)         |
| **特征使用** | **少量、稀疏**的ID类特征                 | **海量、丰富**的用户/物品/上下文/交叉特征  |
| **评估指标** | Recall@K, HitRate@K, Coverage            | **AUC, NDCG, CTR (线上)**                  |

最终回答:

“所以，召回阶段和排序阶段，在我的项目中各自扮演着不可或预的角色。召回负责保证效率和可能性，它像一个漏斗的宽口，确保所有可能的好东西都能被纳入考虑范围；而排序则负责保证效果和精准度，它像漏斗的窄口，对这些可能性进行精细的筛选和打磨。

**如果没有召回，排序就无法在毫秒级的时间内完成；而如果没有排序，仅靠召回的粗糙结果，也无法给用户提供真正精准的、个性化的体验。** 它们二者共同协作，才构成了现代推荐系统高效且精准的核心架构。”



首先，我想先澄清最重要的一点：**在构建共现矩阵时，我们并没有‘正负样本’的概念。**

“正负样本”是用于**模型训练**的，比如在训练Word2Vec或矩阵分解模型时，我们需要通过正负样本来告诉模型应该学习什么、避免什么。

而共现矩阵的构建是一个**纯粹的统计和计数过程**，不是模型训练。它只关心“**符合我们所定义规则的配对**”（您可以将其理解为“正样本”）**出现了多少次**，所有不符合规则的配对，我们直接忽略，不把它们当作负样本来处理。

现在，我来为您详细解释我们都构建了什么样的矩阵，以及它们各自的规则。

------



### 面试回答范例

# 4.共现矩阵都构建了什么？

**面试官：** “你们构建了多种共现矩阵，能具体讲讲它们都是基于什么规则构建的吗？”

**你：** “好的。我们构建了超过15种共现矩阵，它们都基于同一个核心思想：**‘在一次会话（Session）中共同出现过的物品是相关的’**。不同矩阵之间的区别，就在于它们**如何定义‘共同出现’**，也就是它们遵循的**构建规则**不同。

这些规则大致可以分为四大类：



#### **第一类：基于“行为序列”的规则**



这类规则关注物品在会话中出现的**先后顺序**。

- **严格相邻矩阵 (`gpu-217`)**:
  - **规则**：只统计在会话中**顺序紧挨着**的两个物品。
  - **例子**：如果一个会话是 `[A, B, C]`，那么这个矩阵只会统计 `(A,B)` 和 `(B,C)` 这两个关联对。
  - **目的**：捕捉用户最直接的兴趣流转，比如“看了A，立刻就想看B”。
- **先后关系矩阵 (`gpu-116`)**:
  - **规则**：只统计 `物品B` 出现在 `物品A` **之后**的物品对（`ts_y > ts_x`）。
  - **例子**：在 `[A, B, C]` 中，会统计 `(A,B)`, `(A,C)` 和 `(B,C)`。
  - **目的**：构建一个有向的关联图，用于预测用户接下来的行为。



#### **第二类：基于“用户意图”的规则**



这类规则关注用户的行为类型，特别是能代表高购买意图的**“加购”**和**“下单”**。

- **高意图序列矩阵 (`gpu-116`)**:
  - **规则**：在“先后关系矩阵”的基础上，增加了**一个条件**：`物品B` 的行为必须是**“加购”或“下单”**。
  - **例子**：如果会话是 `[A(点击), B(加购), C(点击)]`，那么在 `(A,B)`, `(A,C)`, `(B,C)` 这几个先后关系对中，只有 `(A,B)` 符合规则，因为B是被加购的。
  - **目的**：专门发现那些能够引导用户产生高价值转化的物品关联。
- **纯高意图共现矩阵 (`matrix_12__20`)**:
  - **规则**：在计算前，**直接过滤掉所有“点击”事件**，只在“加购”和“下单”的物品之间建立关联。
  - **例子**：在 `[A(点击), B(加购), C(下单)]` 中，A被忽略，只统计 `(B,C)` 之间的关联。
  - **目的**：构建一个纯粹由高购买意图驱动的关联网络。



#### **第三类：基于“时间因素”的规则**



这类规则认为，关联强度会随时间变化。

- **时间衰减矩阵 (`gpu-93`, `matrix_123_temporal_20`)**:
  - **规则**：对关联强度进行加权。两个物品在一次会话中**交互的时间间隔越短**，或者这次会话**距离现在越近**，它们之间的关联权重就越高。
  - **目的**：让推荐结果更偏向用户的即时兴趣，并能快速捕捉到全局的流行趋势。



#### **第四类：基于“行为类型加权”的规则**



这类规则直接在权重计算中体现不同行为的重要性。

- **订单加权矩阵 (`matrix_123_type136_20`)**:
  - **规则**：为不同类型的目标行为赋予不同权重，比如 `{'点击': 1, '加购': 3, '下单': 6}`。
  - **例子**：如果 `(A, B)` 和 `(A, C)` 各出现一次，但B是被点击，C是被下单，那么 `(A,C)` 的关联分会是 `(A,B)` 的6倍。
  - **目的**：使召回结果更倾向于我们更看重的业务目标（如下单）。

**总结一下就是：**

> “我们构建共现矩阵的过程，是一个**基于规则的统计过程**，而非基于正负样本的模型训练过程。我们定义了**四大类规则**——基于行为序列、用户意图、时间因素和行为类型加权——来从不同角度捕捉物品的关联性。例如，有的矩阵只统计严格相邻的物品，有的只关注能导向购买的序列。通过这种方式，我们为每种关联模式都构建了一个‘专家’矩阵，最终形成了一个强大且多样化的召回引擎。”



# 5.冷启动是如何实现的？

当面试官问：“你项目中的冷启动召回是如何实现的？它为什么能召回那些不活跃的物品？”

你可以这样回答：

“好的。在我的项目中，冷启动召回是多路召回策略中非常重要的一环 ，我设计了一个**两阶段的管道**来专门解决这个问题。第一阶段是**‘基于内容Embedding的候选生成’**，第二阶段是**‘基于多维规则的精细化过滤’**。

这个流程之所以能召回不活跃的物品，其核心在于它**完全摆脱了对用户行为数据的依赖，而是转为利用物品自身的内容信息**。”

### **第一阶段：候选生成 (来自 `4.recall.ipynb`)**

“首先，我们需要一个能够‘看见’冷启动物品的机制。传统的协同过滤（ItemCF/UserCF）是做不到的，因为它们依赖用户行为。

- **实现方式**: 我的解决方案是，在离线阶段，利用**内容Embedding相似度矩阵 (`emb_i2i_sim.pkl`)** 来进行候选物品的生成。
  1. 我会遍历所有用户，并获取他们的历史点击序列。
  2. 对于历史序列中的每一篇文章，我都会从`emb_i2i_sim.pkl`中，查询出与它**内容最相似**的Top-K篇文章（在我的代码中K设得比较大，为150）。
  3. 将这些由内容相似度触发的物品合并起来，就构成了一个**庞大且原始**的候选池（`cold_start_items_raw_dict.pkl`）。
- **为什么能看到不活跃物品？**: 因为**内容Embedding**的计算不依赖于任何点击、购买等用户行为。只要一篇新文章被发布，哪怕它从未被任何人点击过，我们依然可以分析它的文本、标题等内容，为它生成一个Embedding向量。只要它的内容与某篇热门文章相似，它就能在这个阶段被“发现”并“召回”，进入候选池。”

### **第二阶段：规则过滤 (来自 `5.feature_engineering.ipynb`)**

“第一阶段产生的候选池非常庞大，但也很粗糙。第二阶段的目标，就是通过一系列精细的规则，从这个池子里**‘淘金’**，筛选出最适合推荐给用户的、真正的冷启动物品。”

- **实现方式**: 我的`cold_start_items`函数 会对候选池中的每一篇文章，进行一个“四重关卡”的检验：

  1. **关卡一：主题一致性过滤**: 候选文章的主题，必须在用户历史看过的**主题范围**之内。这保证了内容的相关性。
  2. **关卡二：真·冷启动过滤 (最关键)**: 候选文章**必须不能出现在全局的用户点击日志中 (`click_article_ids_set`)**。这是一个“硬规则”，它直接剔除了所有已经被点击过的热门或温启动物品。
  3. **关卡三：用户习惯过滤**: 候选文章的**字数**，必须与用户历史阅读文章的**平均字数**相差不大（如在200字以内）。这保证了内容形态符合用户的偏好。
  4. **关卡四：时间上下文过滤**: 候选文章的**发布时间**，与用户最近一次点击文章的发布时间，相差不能太远（如90天以内）。这保证了推荐的时效性。

- **为什么能召回不活跃物品？**: “经过这套流程，特别是**‘关卡二：真·冷启动过滤’**，我们不仅是‘可以’召回不活跃物品，而是**‘只’留下了那些曝光度极低甚至为零的、真正意义上的冷启动物品**。

  **举个例子**：

  - 假设第一阶段为小王召回了100篇文章，其中包括了热门的`文章A`（已被点击1000次）和全新的`文章B`（被点击0次）。
  - 在第二阶段过滤时，`文章A`会因为 `item in click_article_ids_set` 这个条件为真而被**过滤掉**。
  - 而`文章B`则因为满足“未被点击”这个条件，并且假设它也满足主题、字数、时间等其他规则，最终得以**保留**在推荐列表中。

### **总结**

“所以，我这套冷启动召回策略，它的核心逻辑是：

1. **靠‘内容’破冰**: 利用不依赖用户行为的**内容Embedding相似度**，确保了新物品、不活跃物品有被‘看见’的机会。
2. **靠‘规则’筛选**: 通过一系列精细的业务规则，特别是**‘未被点击过’**这条硬性规定，确保了最终输出的，是精准匹配给潜在兴趣用户的、高质量的**‘首发’推荐**。

这套‘算法+规则’结合的管道，有效地解决了物品冷启动问题，为新内容创造了公平的曝光机会，也提升了整个推荐生态的多样性。”