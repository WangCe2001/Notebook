#### **项目一：新闻推荐系统**



**问题 1：项目中提到了 Stacking 融合，具体是怎么做的？用了什么模型？**

- **您的回答总结**：您提到使用逻辑回归（Logistic Regression）作为元模型（meta-model），将三个排序模型（LGBM Ranker, LGBM Classifier, DIN）的输出作为特征，通过 K-Fold 交叉验证来生成训练数据，以防止信息泄露，最终让逻辑回归模型自动学习各个基模型的权重。
- **回答评估**：回答的整体思路正确，提到了 Stacking 的核心思想（使用一个模型去学习其他模型的输出）和关键技术点（K-Fold 防止泄露），这是一个亮点。但在具体如何使用逻辑回归、输入哪些特征方面略显模糊。
- **参考答案**： 在我的项目中，Stacking 融合的目标是结合多个不同排序模型的优点，以达到比单一模型更好的效果。具体实现如下：
  1. **基模型 (Base Models)**：我选择了三个模型作为基模型：LGBM Ranker、LGBM Classifier 和 DIN 模型。它们从不同角度对物品进行排序，Ranker 关注排序顺序（Listwise），Classifier 关注点击概率（Pointwise），而 DIN 关注用户动态兴趣。
  2. **元模型 (Meta-model)**：我选用逻辑回归作为元模型。它结构简单、训练速度快，并且可解释性强，适合用于学习如何最佳地结合基模型的预测结果。
  3. **特征工程**：为了训练元模型，我需要为每一条训练数据生成新的特征。这里最关键的一步是**通过 K-Fold 交叉验证来防止数据泄露**。具体来说：
     - 我将训练集分为 K 份（例如 K=5）。
     - 进行 K 轮迭代，在每一轮中，我使用 K-1 份数据训练三个基模型，然后对剩下的一份数据进行预测，得到每个样本对应的三个模型的预测分数。
     - K 轮结束后，整个训练集的每个样本都有了由“未见过”它的模型所给出的预测分数。这些分数，连同原始特征，或者仅这些分数本身，就构成了元模型的训练特征。
  4. **模型训练**：最后，我使用 K-Fold 生成的特征和真实的标签（是否点击）来训练逻辑回归元模型。在预测时，先用三个基模型对新样本进行预测，然后将它们的预测结果输入到训练好的逻辑回归模型中，得到最终的排序分数。

------

**问题 2：为什么选择逻辑回归做 Stacking？它有什么优缺点？有没有考虑过其他方法？**

- **您的回答总结**：您提到主要是做了一下实验，另一种方式是人工调参，类似于穷举。
- **回答评估**：这个回答较弱，没有从模型特性和场景需求上解释选择的原因。面试官希望听到您对技术选型背后逻辑的深入思考。
- **参考答案**： 我选择逻辑回归作为元模型主要基于以下几点考虑：
  - **优点**：
    1. **简单高效**：逻辑回归模型简单，计算开销小，训练速度非常快，非常适合作为 Stacking 的第二层。
    2. **可解释性强**：训练完成后，我们可以通过查看逻辑回归的权重，直观地了解每个基模型对于最终结果的贡献度，这有助于我们分析和迭代基模型。
    3. **鲁棒性**：逻辑回归对输入特征的线性组合进行建模，结果相对稳定，不容易过拟合，特别是当基模型已经很复杂时，使用一个简单的元模型可以降低整体的过拟合风险。
  - **缺点**：
    1. **模型能力有限**：逻辑回归是一个线性模型，它无法学习到基模型输出之间复杂的非线性关系。
  - **其他方法的考虑**： 是的，我也考虑过其他模型。例如，可以使用像 **GBDT (梯度提升决策树)** 或 **浅层神经网络** 作为元模型。这些模型能够捕捉特征间更复杂的非线性关系，可能会带来性能上的提升。但它们的缺点是模型更复杂，训练成本更高，且可解释性不如逻辑回归。在当时的项目阶段，逻辑回归已经带来了显著的效果提升，并且其高效和高可解释性的优点非常突出，因此我最终选择了它。

------

**问题 3：逻辑回归模型的原理是什么？损失函数是什么？**

- **您的回答总结**：您正确地将其描述为“一个全连接层 + Sigmoid 激活函数”，并将损失函数确定为“二元交叉熵”。

- **回答评估**：回答准确，展示了您对基础模型原理的理解。

- **参考答案**： 逻辑回归本质上是一个用于二分类的线性模型。它的核心思想是：

  1. 首先，对输入特征进行线性加权求和，即$ z=w^Tx+b$。

  2. 然后，将这个线性结果输入到一个 Sigmoid 函数中，即 $σ(z)=\frac{1}{1+e^{−z}}$。Sigmoid 函数可以将任意实数映射到 (0, 1) 区间，这个输出值可以被看作是样本属于正类的概率。

  3. 它的损失函数是**二元交叉熵 (Binary Cross-Entropy)**，公式为：

     $L(y,y^p)=−[ylog(y^p)+(1−y)log(1−y^p)]$

     其中，y 是真实标签（0或1），y^ 是模型预测的概率。这个损失函数衡量了真实分布和预测分布之间的差异。当模型预测正确时（例如，y=1 且 y^ 接近1），损失值很小；当预测错误时，损失值会变得很大，从而驱动模型在训练过程中通过梯度下降等方法更新权重 w 和偏置 b，以最小化损失。

------

**问题 4：在做 K-Fold 交叉验证时，你是如何划分数据集的？面试官追问：直接按用户ID划分，用户的行为分布可能不均，这会导致问题吗？**

- **您的回答总结**：您承认是简单地按用户 ID 进行划分，并且未深入考虑其可能带来的问题。
- **回答评估**：诚实地承认未考虑周全是一个可接受的回答，但如果能进一步分析其潜在影响并提出改进方案会更好。
- **参考答案**： 在项目中，我最初是按照用户 ID 的范围来粗略地划分数据集的。我理解这种划分方式存在一些潜在问题。
  - **潜在问题**：不同用户群体的行为模式（如活跃度、兴趣广度）可能存在巨大差异。如果简单地按 ID 划分，可能导致某几折（Fold）中恰好聚集了大量的活跃用户，而另几折主要是低活用户。这样训练出来的模型可能存在偏差（Bias），在某些用户群体上表现好，但在另一些上表现差，影响了模型的泛化能力。
  - **改进方案**：更严谨的做法是采用**分层抽样 (Stratified Sampling)**。例如，我们可以先根据用户的行为活跃度（如交互次数）将用户分为高、中、低活等级，然后在每个等级中进行随机抽样，确保每一折（Fold）都包含了各种活跃度用户的样本，并且比例与整体训练集保持一致。这样可以保证每一折数据的分布都更接近于原始数据的分布，从而训练出泛化能力更强的模型。

------

**问题 5：如何评估召回策略的好坏？只看召回率（Recall）有什么问题？**

- **您的回答总结**：您提到主要看召回率，即有多少真实点击的物品被召回了。在面试官的引导下，您也意识到对于不同活跃度的用户，这个指标可能不公平。
- **回答评估**：面试官指出的问题非常关键。只看召回率是一个常见的评估误区。
- **参考答案**： 评估召回策略的效果需要一个综合的指标体系，不能仅仅依赖召回率。
  - **召回率 (Recall)**：这是最基础的指标，计算公式为：**（被召回的相关物品数量）/（所有相关物品的总数量）**。它衡量了召回算法“找得全不全”的能力。
  - **只看召回率的问题**：
    1. **忽略准确性**：召回率高可能只是因为召回的物品数量非常多，但其中可能包含了大量用户不感兴趣的物品，这会给排序阶段带来巨大的压力。
    2. **用户活跃度偏差**：正如面试官所说，对于只有一个点击行为的低活用户，只要召回了这一个物品，召回率就是100%。而对于有1000个点击行为的高活用户，即使召'回了900个，召回率也只有90%。这显然是不公平的。
    3. **忽略新颖性和多样性**：召回策略还需要考虑推荐的物品是否新颖、种类是否丰富，这些是召回率无法衡量的。
  - **更全面的评估指标**：
    1. **精确率 (Precision @ K)**：在召回的前 K 个物品中，有多少是用户真正感兴趣的。这衡量了“找得准不准”。
    2. **命中率 (Hit Rate @ K)**：有多少用户的真实交互物品出现在了召回的前 K 个结果中。这个指标对每个用户一视同仁，在一定程度上缓解了活跃度偏差问题。
    3. **覆盖率 (Coverage)**：召回的物品集合占总物品库的比例，衡量了推荐的多样性和发现新物品的能力。 在实际工作中，我们通常会综合考量这些指标，并结合线上 AB 测试的业务指标（如点击率、转化率）来最终评判一个召回策略的优劣。

------

**问题 6：解释一下 NDCG 指标。**

- **您的回答总结**：您正确地解释了 NDCG 的核心思想，包括 DCG（考虑了位置衰减的累计增益）和 IDCG（理想情况下的 DCG），以及最终的归一化。

- **回答评估**：回答得非常好，准确且清晰。

- **参考答案**： NDCG (Normalized Discounted Cumulative Gain) 是一个衡量排序质量的常用指标，特别适合评估搜索和推荐的排序列表。它的计算分为三步：

  1. **CG (Cumulative Gain)**：累计增益，只计算排序列表中所有相关物品的相关性得分之和，不考虑位置。

  2. **DCG (Discounted Cumulative Gain)**：折损累计增益。它在 CG 的基础上引入了位置衰减，认为排在越前面的物品价值越高。公式为：

     DCGp=i=1∑plog2(i+1)reli

     其中，reli 是位置 i 上的物品的相关性得分，p 是列表的长度。

  3. **NDCG (Normalized DCG)**：归一化折损累计增益。因为不同查询或用户的相关物品数量不同，DCG 的值没有一个统一的上限，难以进行比较。因此，我们用当前列表的 DCG 除以理想排序列表的 DCG（即 IDCG），将结果归一化到 [0, 1] 区间。

     NDCGp=IDCGpDCGp

     NDCG 不仅考虑了物品是否相关，还考虑了相关物品的排名顺序，是一个非常全面和鲁棒的排序评估指标。

------



#### **通用技术问题**



**问题 7：什么是过拟合？如何解决？**

- **您的回答总结**：您提到了定义（训练集效果好，测试集效果差）、原因（数据量小）和解决方法（增大数据量、正则化、Dropout）。
- **回答评估**：回答覆盖了关键点，比较全面。
- **参考答案**：
  - **定义**：过拟合是指机器学习模型在训练数据上表现得过于优秀，以至于学习到了训练数据中的噪声和 случайные 波动，但对未见过的、新的测试数据泛化能力很差的现象。
  - **原因**：
    1. **数据量太小**：数据量不足以支撑模型学习到普适的规律。
    2. **模型过于复杂**：模型的参数过多，复杂度远超解决问题所需。
    3. **特征数量过多**：特征维度过高，存在冗余或不相关的特征。
  - **解决方法**：
    1. **增加数据量**：这是最根本有效的方法。可以通过数据增强（如图像的旋转、裁剪）来扩充数据。
    2. **降低模型复杂度**：选择更简单的模型，或者减少网络层数、神经元数量。
    3. **正则化 (Regularization)**：在损失函数中加入惩罚项，限制模型参数的大小。
       - **L1 正则化**：倾向于让权重参数变得稀疏（很多为0），可以用于特征选择。
       - **L2 正则化**：让权重参数更平滑，分布更均匀，防止个别权重过大。
    4. **Dropout**：在神经网络训练过程中，随机地让一部分神经元失活（输出为0），强迫网络学习到更鲁棒的特征，有效防止神经元之间的协同适应。
    5. **早停 (Early Stopping)**：在训练过程中监控验证集的性能，当性能不再提升甚至开始下降时，就提前终止训练。

------

**问题 8 (编程题)：翻转一棵二叉树。**

- **您的回答总结**：您最初想到了层序遍历，但最终采用并解释了更简洁的递归（前序遍历）方法，并在编写代码时现场调试修正了一个小错误。

- **回答评估**：思路清晰，最终给出的递归解法是这道题的标准解法之一。能在面试官面前快速定位并修正代码错误，也体现了良好的编码和调试能力。

- **参考答案 (C++ 递归解法)**：

  C++

  ```python
  /**
   * Definition for a binary tree node.
   * struct TreeNode {
   * int val;
   * TreeNode *left;
   * TreeNode *right;
   * TreeNode() : val(0), left(nullptr), right(nullptr) {}
   * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
   * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
   * };
   */
  class Solution {
  public:
      TreeNode* invertTree(TreeNode* root) {
          // 递归的终止条件：如果节点为空，直接返回
          if (root == nullptr) {
              return nullptr;
          }
  
          // 交换当前节点的左右子节点
          TreeNode* temp = root->left;
          root->left = root->right;
          root->right = temp;
  
          // 递归地翻转左子树和右子树
          invertTree(root->left);
          invertTree(root->right);
  
          // 返回翻转后的根节点
          return root;
      }
  };
  ```



### **总体面试表现与建议**



1. **优点**：
   - **项目经验扎实**：您对自己做的项目非常熟悉，能够清晰地描述推荐系统的“召回-排序”架构和所使用的具体技术。
   - **基础知识牢固**：对于逻辑回归、NDCG、过拟合等核心概念，您的理解是准确的。
   - **沟通表达清晰**：能够流畅地表达自己的思路。
   - **编码能力良好**：在编程题环节，思路正确，能够快速完成。
2. **可提升之处**：
   - **深化对“为什么”的思考**：在多个问题上（如“为什么用逻辑回归”、“为什么只看召回率”），您的回答停留在“我用了它”，而缺乏对“为什么是它，而不是别的”的深入探讨。这是从“会用”到“精通”的关键一步。建议在准备项目时，多问自己几个“为什么”，并横向对比不同技术的优劣和适用场景。
   - **注意细节和严谨性**：K-Fold 的划分方式、召回的评估指标等问题，都考验了您在处理实际问题时的严谨性。在描述项目时，可以主动提及自己是如何考虑并解决这些细节问题的，会显得您考虑更周全。
   - **扩展知识广度**：当被问到不熟悉的领域（如CV、其他评估指标）时，可以坦诚表示不熟悉，但也可以尝试从已知知识出发进行类比或推测，展示自己的学习和思考能力。

总的来说，这是一次成功的面试，您展现了作为一名准算法工程师的良好潜质。通过针对性地加强对技术选型背后原理的理解和对细节问题的严谨思考，您在未来的面试中会表现得更加出色。



# 常见激活函数和损失函数的优缺点

### **第一部分：激活函数 (Activation Functions)**



激活函数的主要作用是向神经网络中引入**非线性因素**。如果没有激活函数，无论神经网络有多少层，其本质上都只是一个线性模型，无法学习和拟合复杂的数据模式。



#### **1. Sigmoid 函数**



- **公式**: 

  σ(x)=1+e−x1

- **函数图像**: S 型曲线，输出范围在 (0, 1) 之间。

- **优点**:

  - **输出范围有限**：输出值在 0 到 1 之间，非常适合用作二分类任务的输出层，可以表示概率。
  - **平滑可导**：函数处处光滑，方便求导，适用于梯度下降算法。

- **缺点**:

  - **梯度消失 (Vanishing Gradients)**：当输入值非常大或非常小时，函数的导数（梯度）接近于 0。在深度网络中，这会导致反向传播时梯度逐层递减，使得深层网络的权重几乎无法更新。这是它最主要的问题。
  - **输出非零中心**：输出值恒为正，这会导致后续层的输入是非零中心的，可能降低梯度下降的收敛速度。
  - **计算成本高**：包含指数运算（`e^x`），计算上比 ReLU 等函数更耗时。



#### **2. Tanh (双曲正切) 函数**



- **公式**: 

  tanh(x)=ex+e−xex−e−x

- **函数图像**: 与 Sigmoid 类似的 S 型曲线，但输出范围在 (-1, 1) 之间。

- **优点**:

  - **输出是零中心**：输出范围为 (-1, 1)，均值为 0，解决了 Sigmoid 输出非零中心的问题，通常收敛速度比 Sigmoid 快。
  - **平滑可导**：同样是光滑可导的。

- **缺点**:

  - **梯度消失**：仍然存在梯度消失的问题，当输入饱和时（接近 -1 或 1），梯度也会趋近于 0。
  - **计算成本高**：同样包含指数运算。



#### **3. ReLU (Rectified Linear Unit, 修正线性单元)**



- **公式**: 

  ReLU(x)=max(0,x)

- **函数图像**: 在 x < 0 时为 0，在 x > 0 时为一条斜率为 1 的直线。

- **优点**:

  - **解决梯度消失问题**：在正数区间的梯度恒为 1，有效缓解了梯度消失问题，使得深度网络训练成为可能。
  - **计算速度极快**：只涉及一个简单的阈值判断，没有复杂的指数运算。
  - **稀疏性**：当输入为负时，输出为 0，这会使网络中的一部分神经元失活，形成稀疏表示，减少了参数的相互依赖关系，可能有助于防止过拟合。

- **缺点**:

  - **Dying ReLU 问题**：如果一个神经元的输入在训练中一直为负，那么它的梯度将永远是 0，导致该神经元的权重无法再更新。这个神经元就“死亡”了。
  - **输出非零中心**：和 Sigmoid 一样，输出也非零中心。



#### **4. Leaky ReLU / PReLU**



- **公式**: 

  LeakyReLU(x)=max(αx,x)

- **函数图像**: 与 ReLU 类似，但在 x < 0 时，有一条微小的正斜率（通常 α=0.01）。

- **优点**:

  - **解决 Dying ReLU 问题**：为负输入赋予了一个小的非零梯度，使得神经元在输入为负时也能进行权重更新，不会“死亡”。
  - **保留 ReLU 的所有优点**：计算速度快，无梯度饱和问题。

- **缺点**:

  - **效果不一**：虽然理论上更好，但在实践中，其性能提升不一定总是很明显。
  - **引入新超参数**：Leaky ReLU 中的 α 值需要手动设定。PReLU（Parametric ReLU）将 α 作为一个可学习的参数，但这增加了模型的复杂度。

------



### **第二部分：损失函数 (Loss Functions)**



损失函数用于衡量模型预测值与真实值之间的差距。模型的训练过程就是通过优化算法（如梯度下降）来最小化损失函数的过程。



#### **A. 回归任务 (Regression Tasks)**





#### **1. MSE (Mean Squared Error, 均方误差)**



- **公式**: 

  MSE=n1i=1∑n(yi−y^i)2

  其中 yi 是真实值，y^i 是预测值。

- **优点**:

  - **处处可导**：函数光滑，方便进行梯度下降。
  - **惩罚大误差**：由于使用了平方，它对误差较大的预测给予更大的惩罚，这在某些场景下是合理的。

- **缺点**:

  - **对离群点敏感**：如果数据中存在离群点（Outliers），平方项会使这些点的误差被放大，导致模型被离群点主导，从而产生偏差。



#### **2. MAE (Mean Absolute Error, 平均绝对误差)**



- **公式**: 

  MAE=n1i=1∑n∣yi−y^i∣

- **优点**:

  - **对离群点鲁棒**：相比 MSE，MAE 对离群点的敏感度低得多，因为误差是线性增加的，而不是平方增加。

- **缺点**:

  - **在零点处不可导**：这使得在接近最优值时梯度可能在 0 附近震荡，不利于模型的收敛。在实践中，通常会使用一个固定的较小学习率或分段函数来解决。
  - **梯度恒定**：无论误差大小，梯度都为常数，这可能导致在误差很小时，模型更新的步长依然很大，错过最优点。



#### **B. 分类任务 (Classification Tasks)**





#### **1. 交叉熵损失 (Cross-Entropy Loss)**



- **二元交叉熵 (Binary Cross-Entropy)**：用于二分类任务。

  - **公式**: 

    L=−n1i=1∑n[yilog(y^i)+(1−yi)log(1−y^i)]

    通常与 `Sigmoid` 激活函数配合使用。

- **分类交叉熵 (Categorical Cross-Entropy)**：用于多分类任务。

  - **公式**: 

    L=−n1i=1∑nc=1∑Cyiclog(y^ic)

    其中 C 是类别数，yic 是一个 one-hot 编码的符号（如果样本 i 属于类别 c，则为 1，否则为 0），y^ic 是模型预测样本 i 属于类别 c 的概率。通常与 `Softmax` 激活函数配合使用。

- **优点**:

  - **性能优越**：是分类任务中最常用且效果最好的损失函数。
  - **梯度特性好**：当与 Sigmoid 或 Softmax 结合时，在预测错误且置信度高的情况下会产生很大的梯度，有助于模型快速学习和修正。

- **缺点**:

  - **对标签噪声敏感**：如果训练数据中的标签是错误的（标签噪声），模型会尽力去拟合这个错误标签，可能导致性能下降。



#### **2. Hinge Loss (合页损失)**



- **公式**: 

  L=max(0,1−t⋅y^)

  其中 t 是目标值（通常为 -1 或 1），y^ 是模型的原始输出（未经激活函数）。

- **优点**:

  - **鼓励间隔最大化**：主要用于支持向量机（SVM）。它不仅要求分类正确，还要求预测值与决策边界之间有足够的间隔（margin），这使得模型的泛化能力更好。
  - **对正确分类不惩罚**：对于已经正确分类且间隔足够的样本（t⋅y^≥1），损失为 0，模型不会在这些“简单”样本上花费精力。

- **缺点**:

  - **非概率性输出**：模型的输出不具有概率意义。
  - **在某些点不可导**：对优化算法的要求更高一些。在深度学习中不如交叉熵常用。

希望这份详细的总结能对您有所帮助！