# 1.Transformer

![../_images/transformer.svg](https://typora-picture-wang.oss-cn-shanghai.aliyuncs.com/transformer.svg)

![image-20251010181614153](https://typora-picture-wang.oss-cn-shanghai.aliyuncs.com/image-20251010181614153.png)

## 1.宏观理解

### **学习 Transformer 的结构化路径**

我们可以分四步走：

1. **思想准备：为什么要用 Transformer？** (The "Why")
2. **宏观理解：Transformer 是如何工作的？** (The "How", high-level)
3. **微观拆解：深入架构核心组件** (The "What", low-level)
4. **动手实践：代码实现与应用** (The "Do")

### **第一步：思想准备 (The "Why")**

在深入 Transformer 之前，最重要的问题是：它解决了什么问题？

答案是：**它解决了传统序列模型（如 RNN, LSTM, GRU）的两个核心痛点。**

- **痛点1：无法并行计算。** RNN 类的模型必须按顺序处理数据，比如要处理第 t 个词，必须先处理完第 t−1 个词。这导致模型训练速度很慢，尤其是在处理长序列时。
- **痛点2：长距离依赖问题。** 尽管 LSTM 和 GRU 缓解了 RNN 的梯度消失/爆炸问题，但在处理非常长的序列时（例如一篇长文章），模型仍然很难捕捉到相距很远的词之间的依赖关系。

**Transformer 的革命性思想是：彻底抛弃“循环”结构，完全依赖“注意力机制”（Attention Mechanism）来捕捉序列内的依赖关系。** 这使得模型可以并行处理整个序列，大大提高了效率，并且通过自注意力机制（Self-Attention），可以直接计算序列中任意两个位置之间的关系，完美解决了长距离依赖问题。

**建议：** 在开始前，你可以快速回顾一下 RNN/LSTM 的基本原理和注意力机制。如果你对注意力机制已经有所了解，那将是一个很好的起点。

### **第二步：宏观理解 (The "How")**

在这一步，我们的目标不是搞懂每一个细节，而是建立一个整体的认知框架。

你需要知道 Transformer 主要由两大部分组成：**编码器（Encoder）** 和 **解码器（Decoder）**。

*(图片来源: "Attention Is All You Need" 论文)*

- **编码器 (Encoder) 的作用：** 输入一个序列（比如一个句子），然后为序列中的每个词生成一个富含上下文信息的向量表示（Embedding）。你可以把它想象成一个高级的“特征提取器”。
- **解码器 (Decoder) 的作用：** 接收编码器的输出，并结合已经生成的部分结果，来预测下一个输出。在机器翻译任务中，它会一个词一个词地生成翻译后的句子。

**强烈推荐的入门资源：** Jay Alammar 的博客 **《The Illustrated Transformer》**。这篇文章用非常直观的动图和解释，把 Transformer 的工作流程讲得清晰易懂。这是理解宏观工作原理的“必读”材料。

### **第三步：微观拆解 (The "What")**

这是学习的核心，我们需要像剥洋葱一样，一层一层地拆解它的内部结构。我会列出你需要掌握的关键组件，你可以逐个击破。

#### **1. 输入处理 (Input Processing)**

- **词嵌入 (Token Embedding):** 和其他 NLP 模型一样，将输入的词语转换成向量。
- **位置编码 (Positional Encoding):** 这是个关键！因为 Transformer 没有循环结构，它本身无法感知词语的顺序。所以，我们必须在输入的 Embedding 上加入一个“位置向量”，来告诉模型每个词在句子中的位置信息。这个位置编码通常是用 `sin` 和 `cos` 函数生成的。

#### **2. 编码器核心组件 (Encoder Block)**

一个编码器是由 N 个相同的编码器层（Encoder Layer）堆叠而成的。每个编码器层包含两个核心子层：

- **多头自注意力机制 (Multi-Head Self-Attention):**
  - **这是 Transformer 的灵魂！** 你需要先理解什么是 **自注意力 (Self-Attention)**。它的核心是计算三个向量：查询（Query, Q）、键（Key, K）、值（Value, V）。对于一个输入词，它的 Q 会和句子中所有词的 K 进行点积计算，得到一个注意力分数（Attention Score），这个分数决定了在理解当前词时，应该给予其他词多大的“关注度”。最后，将这些分数与 V 相乘并加权求和，得到该词的新的、融合了全局上下文的表示。
  - 其计算公式为：$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
  - **多头 (Multi-Head)** 指的是我们不只做一次注意力计算，而是将 Q, K, V 投影到多个不同的子空间（“头”），分别计算注意力，然后将结果拼接起来。这允许模型从不同角度关注信息，比如一个头可能关注语法关系，另一个头可能关注语义关系。
- **前馈神经网络 (Feed-Forward Network):**
  - 这是一个简单的全连接神经网络，通常包含两层线性变换和一个 ReLU 激活函数。它在每个词的位置上独立进行，用于对自注意力层的输出进行非线性变换，增加模型的表达能力。
- **残差连接 (Residual Connection) & 层归一化 (Layer Normalization):**
  - 每个子层（自注意力和前馈网络）的输出都会与该子层的输入进行一个“Add & Norm”操作。
  - **Add (残差连接):** 解决了深度网络中的梯度消失问题，让信息流动更顺畅。
  - **Norm (层归一化):** 稳定训练过程，加速收敛。

#### **3. 解码器核心组件 (Decoder Block)**

解码器也由 N 个相同的解码器层（Decoder Layer）堆叠而成。它比编码器层多一个子层：

- **带掩码的多头自注意力 (Masked Multi-Head Self-Attention):**
  - 和编码器的自注意力类似，但增加了一个“掩码”（Mask）。因为在生成第 `i` 个词时，模型不应该“看到”未来的词（`i+1`, `i+2`, ...），所以通过掩码将这些位置的注意力分数设为负无穷，这样在 softmax 后它们的权重就接近于零。
- **编码器-解码器注意力 (Encoder-Decoder Attention):**
  - 这是连接编码器和解码器的桥梁。它的 Q 来自于解码器的上一层，而 K 和 V 则来自于**编码器的最终输出**。这使得解码器在生成每个词时，能够关注到输入序列的所有相关部分。
- **前馈神经网络 (Feed-Forward Network):**
  - 与编码器中的完全相同。
- **残差连接 & 层归一化：**
  - 同样在每个子层后都有应用。

### **第四步：动手实践 (The "Do")**

理论学习后，最好的消化方式就是写代码。

1. **跟读经典教程：**
   - **《The Annotated Transformer》**: 这是哈佛大学 NLP 团队写的一篇博客，它用 PyTorch 逐行实现了 "Attention Is All You Need" 论文中的模型。这对于理解每个组件的细节非常有帮助。
2. **自己动手实现：**
   - 尝试用 PyTorch 或 TensorFlow 从零开始搭建一个简化的 Transformer 模型。不用完全复现论文，可以先实现一个编码器层，再实现一个解码器层，然后把它们组装起来。
3. **应用到实际任务：**
   - 从一个简单的任务开始，比如机器翻译或文本分类。你可以使用 Hugging Face 的 `transformers` 库，这个库提供了预训练好的 Transformer 模型（如 BERT, GPT）和易于使用的 API。通过调用这些模型，你可以更好地理解它们是如何在实际中应用的。

## 2.QKV和多头注意力机制

我会分成三个部分来回答你：

1. **Q, K, V 的含义与作用（类比解释）**
2. **单头注意力（Scaled Dot-Product Attention）的计算过程与公式**
3. **多头注意力（Multi-Head Attention）的进化与作用**

### 1. Q, K, V 的含义与作用 (Query, Key, Value)

为了直观理解，我们先不用公式，用一个**图书馆查资料的例子**来类比。

假设你想写一篇关于“机器学习”的论文，你带着一个具体问题（**Query**）去图书馆。

- **Query (Q) - 查询：** 代表你当前“关注”的焦点。比如，你脑中的问题是：“什么是梯度下降？”。这个 Q 就代表了“梯度下降”这个意图。
- **Key (K) - 键：** 代表图书馆里每本书（或资料）的书名、标签或索引。比如书A的 Key 是“深度学习入门”，书B的 Key 是“优化算法详解”，书C的 Key 是“计算机网络”。
- **Value (V) - 值：** 代表那本书里真正包含的知识内容。书A的内容是深度学习知识，书B的内容是各种优化算法的详细讲解。

**查资料的过程就是注意力的计算过程：**

1. 你拿着你的 **Q** (“梯度下降”)，去和图书馆里所有书的 **K** (书名/标签) 进行匹配，看哪个 K 和你的 Q 最相关。
2. 你发现，你的 Q (“梯度下降”) 和书B的 K (“优化算法详解”) 相似度最高，和书A的 K (“深度学习入门”) 相似度次之，和书C的 K (“计算机网络”) 完全不相关。
3. 这个“相似度”就是 **注意力分数 (Attention Score)**。
4. 根据这个分数，你决定重点阅读书B的内容 (**V**)，次要参考书A的内容 (**V**)，而完全忽略书C的内容 (**V**)。
5. 最终，你脑中关于“梯度下降”的知识，就是所有书籍内容的一个**加权平均**，权重就是你计算出的注意力分数。

在自注意力机制（Self-Attention）中，一个序列里的每个词，都会同时扮演三个角色：

1. 它作为 **Query**，去主动关注序列里的其他词。
2. 它作为 **Key**，去被序列里的其他词关注。
3. 它作为 **Value**，贡献出自己的信息。

**Q, K, V 是怎么来的？** 它们都是由同一个输入向量（比如词 `x_i` 的 embedding）通过不同的线性变换（乘以不同的权重矩阵 WQ,WK,WV）得到的。

- $q_i=x_iW^Q$
- $k_i=x_iW^K$
- $v_i=x_iW^V$

学习这些权重矩阵 $W^Q,W^K,W^V $是模型训练的核心任务之一。这使得模型能够学会如何根据不同的“关注点”（Q）去匹配最合适的“信息标签”（K），并提取出最有价值的“信息内容”（V）。

### 2. 单头注意力 (Scaled Dot-Product Attention)

这是多头注意力的基础。它的计算过程严格遵循我们上面提到的逻辑，并将其数学化。

#### **计算公式**

$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

#### **分步详解**

1. **第一步：计算注意力分数 (Score)**
   - Score=QKT
   - 将每个词的 Query 向量与所有词的 Key 向量进行点积。点积的结果越大，代表 Q 和 K 的相似度越高，也就是“关注度”越高。如果 Q 是一个 n×dk 的矩阵（n个词，每个词维度为 dk），K 是一个 m×dk 的矩阵，那么结果就是一个 n×m 的分数矩阵。
2. **第二步：缩放 (Scale)**
   - $$ScaledScore=\frac{Score}{\sqrt{d_k}}$$
   - 将得到的分数除以$\sqrt{d_k}$（dk 是 Key 向量的维度）。**这一步非常关键**，它的作用是进行“数值稳定”。如果 dk 的维度过大，点积的结果可能会变得非常大，导致 softmax 函数进入梯度极小的区域，使得梯度反向传播时消失，不利于模型训练。除以$\sqrt{d_k}$ 可以将方差稳定在1附近，缓解这个问题。
3. **第三步：归一化 (Softmax)**
   - $$Weights=softmax(ScaledScore)$$
   - 对缩放后的分数按行进行 softmax 操作，使其变成和为 1 的概率分布。这个结果就是我们最终的**注意力权重**，表示对于一个 Query，应该将多少注意力分配给每个 Value。
4. **第四步：加权求和 (Weighted Sum)**
   - $$Output=Weights⋅V$$
   - 将上一步得到的注意力权重矩阵与 Value 矩阵相乘。这本质上就是一个加权求和，用计算出的权重对所有的 Value 进行加权，得到最终的输出。这个输出向量融合了整个序列中所有相关词的信息。

### 3. 多头注意力机制 (Multi-Head Attention)

**它为什么起作用？**

单头注意力只允许模型在一个子空间里学习词与词之间的关系。但词语间的关系是多样的，比如：

- “我**爱**北京天安门”里的“爱”，可能关注的是主语“我”和宾语“北京天安门”。（主谓宾关系）
- “The **animal** didn't cross the street because **it** was too tired.” 中的 “it” 应该关注 “animal”。（指代关系）

只用一组 Q, K, V 去学习所有这些关系，负担太重了。

**多头注意力的思想就是：与其做一次复杂的注意力计算，不如做多次简单的、并在不同子空间里进行的注意力计算，然后把结果综合起来。**

就像是请一个“专家委员会”来做决策，而不是只请一个专家。每个“头”（专家）都可以独立地学习一种关系模式。

#### **计算公式**

$$
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O
$$

其中，每个 headi 的计算方式为：
$$
headi=Attention(QW_i^Q,KW_i^K,VW_i^V)
$$

#### **分步详解**

1. **第一步：线性投影 (Projection)**
   - 对于输入的 Q, K, V，我们不再直接使用它们，而是准备 h 组不同的线性变换矩阵 (WiQ,WiK,WiV，这里 i 从 1到 h，h 是头的数量)。
   - 将 Q, K, V 分别乘以这些矩阵，得到 h 组降维后的 qi,ki,vi。这就好比是把原始问题分解成了 h 个子问题，让每个“头”去不同的“子空间”里分析。
2. **第二步：并行计算注意力 (Parallel Attention)**
   - 对这 h 组 qi,ki,vi **并行地**、**独立地**执行上面讲的单头注意力（Scaled Dot-Product Attention）计算。
   - 这样，我们会得到 h 个输出结果 head1,head2,...,headh。每个 headi 都代表了一个“专家”从自己擅长的角度给出的“意见”。
3. **第三步：拼接 (Concatenate)**
   - 将这 h 个输出结果 headi 在维度上拼接起来。
4. **第四步：再次投影 (Final Projection)**
   - 将拼接后的巨大向量乘以一个最终的权重矩阵 WO，将其投影回模型期望的原始维度。这一步可以看作是“汇总所有专家的意见”，并做出最终的综合决策。

**总结一下：**

- **Q, K, V** 是从输入向量变换而来的三个角色，分别代表**查询、索引、内容**。
- **单头注意力**通过 QK 计算相关性，然后用这个相关性作为权重去对 V 进行加权求和。
- **多头注意力**通过将 QKV 投影到多个不同的子空间，并行地计算多次单头注意力，使得模型能够同时学习到序列中不同类型的依赖关系，最后再将所有“头”的信息整合起来，极大地增强了模型的表达能力。



## 3.多头注意力机制代码

==单头注意力核心==

```python
import torch
from torch import nn
class MultiHeadAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        self.num_heads = num_heads
        self.head_dim = hidden_size 
    
```

